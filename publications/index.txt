1:"$Sreact.fragment"
2:I[3719,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-627ef85ed6aeb379.js"],"ThemeProvider"]
3:I[768,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-627ef85ed6aeb379.js"],"default"]
4:I[7555,[],""]
5:I[1295,[],""]
6:I[2548,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-627ef85ed6aeb379.js"],"default"]
8:I[9665,[],"MetadataBoundary"]
a:I[9665,[],"OutletBoundary"]
d:I[4911,[],"AsyncMetadataOutlet"]
f:I[9665,[],"ViewportBoundary"]
11:I[6614,[],""]
:HL["/_next/static/css/9904c70f5604abd5.css","style"]
0:{"P":null,"b":"OD1hNuFOBOxmKMsl6VYRN","p":"","c":["","publications",""],"i":false,"f":[[["",{"children":[["slug","publications","d"],{"children":["__PAGE__",{}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/9904c70f5604abd5.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"scroll-smooth","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","href":"/favicon.svg","type":"image/svg+xml"}],["$","link",null,{"rel":"dns-prefetch","href":"https://google-fonts.jialeliu.com"}],["$","link",null,{"rel":"preconnect","href":"https://google-fonts.jialeliu.com","crossOrigin":""}],["$","link",null,{"rel":"preload","as":"style","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}],["$","link",null,{"rel":"stylesheet","id":"gfonts-css","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap","media":"print"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              (function(){\n                var l = document.getElementById('gfonts-css');\n                if (!l) return;\n                if (l.media !== 'all') {\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\n                }\n              })();\n            "}}],["$","noscript",null,{"children":["$","link",null,{"rel":"stylesheet","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}]}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              try {\n                const theme = localStorage.getItem('theme-storage');\n                const parsed = theme ? JSON.parse(theme) : null;\n                const setting = parsed?.state?.theme || 'system';\n                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\n                var root = document.documentElement;\n                root.classList.add(effective);\n                root.setAttribute('data-theme', effective);\n              } catch (e) {\n                var root = document.documentElement;\n                root.classList.add('light');\n                root.setAttribute('data-theme', 'light');\n              }\n            "}}]]}],["$","body",null,{"className":"font-sans antialiased","children":["$","$L2",null,{"children":[["$","$L3",null,{"items":[{"title":"About","type":"page","target":"about","href":"/"},{"title":"Publications","type":"page","target":"publications","href":"/publications"},{"title":"Internship","type":"page","target":"internship","href":"/internship"},{"title":"Awards","type":"page","target":"awards","href":"/awards"},{"title":"Services","type":"page","target":"services","href":"/services"},{"title":"CV","type":"page","target":"cv","href":"/cv"}],"siteTitle":"Zijing Cai","enableOnePageMode":false}],["$","main",null,{"className":"min-h-screen pt-16 lg:pt-20","children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","$L6",null,{"lastUpdated":"November 27, 2025"}]]}]}]]}]]}],{"children":[["slug","publications","d"],["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L7",["$","$L8",null,{"children":"$L9"}],null,["$","$La",null,{"children":["$Lb","$Lc",["$","$Ld",null,{"promise":"$@e"}]]}]]}],{},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","ZulEMqoLrP0Ncd4PdZwbt",{"children":[["$","$Lf",null,{"children":"$L10"}],null]}],null]}],false]],"m":"$undefined","G":["$11","$undefined"],"s":false,"S":true}
12:"$Sreact.suspense"
13:I[4911,[],"AsyncMetadata"]
15:I[6669,["17","static/chunks/17-9c04c9413a9f9f1f.js","178","static/chunks/178-595a94b9af1e67b5.js","748","static/chunks/748-1f3129a1e6365cf9.js","182","static/chunks/app/%5Bslug%5D/page-cf30c519aa4efa8f.js"],"default"]
9:["$","$12",null,{"fallback":null,"children":["$","$L13",null,{"promise":"$@14"}]}]
16:T583,The concurrent use of multiple medications to treat one or more diseases is prevalent. Identifying N-ary drug combinations from biomedical texts aids in uncovering significant pharmacological effects triggered by drug-drug interactions. Previous methods for this emerging task have primarily concentrated on representing drug entities using pre-trained language models, overlooking the comprehensive extraction of contextual and task-specific semantic information. To address these limitations, we develop a semantic fusion method grounded in machine reading comprehension (MRC) framework. Our model, termed Reading Comprehension powered semantic Fusion network for Identification of N-ary Drug combinations (RCFIND), first constructs relevant contexts and queries for each individual drug combination. Then, diverse information sources, including task-specific semantics, drug entity representations and contextual details, are fused by using a simplified Capsule network as well as incorporating contrastive learning. We assess RCFIND, achieving F1 scores ranging from 72.0% to 83.3% across four types of evaluations. Experimental results demonstrate significant performance enhancements over existing baselines, with at least a 5% F1 score improvement. Ablation studies and further analysis confirm the efficacy of the MRC framework and contrastive learning in accurately identifying N-ary drug combinations.17:T797,@article{zhang2025reading,
  title = {Reading comprehension powered semantic fusion network for identification of N-ary drug combinations},
  author = {Zhang, Hua and Zhan, Peiqian and Yang, Cheng and Yan, Yongjian and Cai, Zijing and Shan, Guogen and Jiang, Bo and Chen, Bi and Gu, Qing and Zhou, Qingqing},
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {144},
  pages = {110096},
  year = {2025},
  publisher = {Elsevier},
  abstract = {The concurrent use of multiple medications to treat one or more diseases is prevalent. Identifying N-ary drug combinations from biomedical texts aids in uncovering significant pharmacological effects triggered by drug-drug interactions. Previous methods for this emerging task have primarily concentrated on representing drug entities using pre-trained language models, overlooking the comprehensive extraction of contextual and task-specific semantic information. To address these limitations, we develop a semantic fusion method grounded in machine reading comprehension (MRC) framework. Our model, termed Reading Comprehension powered semantic Fusion network for Identification of N-ary Drug combinations (RCFIND), first constructs relevant contexts and queries for each individual drug combination. Then, diverse information sources, including task-specific semantics, drug entity representations and contextual details, are fused by using a simplified Capsule network as well as incorporating contrastive learning. We assess RCFIND, achieving F1 scores ranging from 72.0% to 83.3% across four types of evaluations. Experimental results demonstrate significant performance enhancements over existing baselines, with at least a 5% F1 score improvement. Ablation studies and further analysis confirm the efficacy of the MRC framework and contrastive learning in accurately identifying N-ary drug combinations.},
  doi = {https://doi.org/10.1016/j.engappai.2025.110096}
}18:T4f1,Short text classification task poses challenges in natural language processing due to insufficient contextual information. This task is typically approached by extracting rich semantic features in the text and encoding it as a sentence-level representation using deep neural networks. The self-attention mechanism has emerged as one of the primary methods to tackle this problem. However, traditional attention methods only focus on the interactions between tokens, neglecting the semantic relationships between features. We propose a novel attention-based module, called token-feature woven attention fusion (TFWAF) network for sentence-level representation information aggregation, which leverages the self-attention mechanism from both token and feature perspectives. Moreover, we design a multi-schema prompting approach within machine reading comprehension and prompt learning paradigms to better utilize prior knowledge in a pre-trained language model and recognize enhanced textual semantic representation. Experimental results show our model achieves state-of-the-art performance compared to existing baselines on eight benchmark datasets in the context of short text classification. The source code is available in https://github.com/Aaronzijingcai/MP-TFWA19:T6a0,@article{cai2024multi,
  title = {Multi-schema prompting powered token-feature woven attention network for short text classification},
  author = {Cai, Zijing and Zhang, Hua and Zhan, Peiqian and Jia, Xiaohui and Yan, Yongjian and Song, Xiawen and Xie, Bo},
  journal = {Pattern Recognition},
  volume = {156},
  pages = {110782},
  year = {2024},
  publisher = {Elsevier},
  abstract = {Short text classification task poses challenges in natural language processing due to insufficient contextual information. This task is typically approached by extracting rich semantic features in the text and encoding it as a sentence-level representation using deep neural networks. The self-attention mechanism has emerged as one of the primary methods to tackle this problem. However, traditional attention methods only focus on the interactions between tokens, neglecting the semantic relationships between features. We propose a novel attention-based module, called token-feature woven attention fusion (TFWAF) network for sentence-level representation information aggregation, which leverages the self-attention mechanism from both token and feature perspectives. Moreover, we design a multi-schema prompting approach within machine reading comprehension and prompt learning paradigms to better utilize prior knowledge in a pre-trained language model and recognize enhanced textual semantic representation. Experimental results show our model achieves state-of-the-art performance compared to existing baselines on eight benchmark datasets in the context of short text classification. The source code is available in https://github.com/Aaronzijingcai/MP-TFWA},
  doi = {10.1016/j.patcog.2024.110782}
}1a:T5be,The primary challenge in multimodal sentiment analysis (MSA), which utilizes textual, audio, and visual information to analyze speakers' emotions, lies in constructing representation vectors that incorporate both unimodal semantic and multimodal interaction information. While existing research has extensively focused on multimodal fusion strategies, there remains insufficient exploration of the intrinsic potential within concurrently enhancing unimodal and multimodal representations. To address this gap, we introduce two additional steps to traditional three-step MSA: text modality enhancement through the machine reading comprehension (MRC) framework and multimodal representation reconstruction via proposing the diverse diffusion denoising autoencoder (D3AE) module. The MRC queries are integrated to locate sentiment-related prior knowledge, thereby deepening textual semantic understanding from a pretrained language model. Meanwhile, D3AE employs a single-step denoising strategy along with diffusion models across multiple time intervals, enabling efficient reconstruction and enhancement of multimodal representations. Extensive experiments conducted on two benchmark datasets, CMU-MOSI and CMU-MOSEI, validate that our model, MRC-D3AE, achieves state-of-the-art performance. The superiority of our model over existing baselines is primarily attributed to integrating MRC for enhancing text modality and D3AE for reconstructing multimodal representations.1b:T785,@article{zhang2024reconstructing,
  title = {Reconstructing representations using diffusion models for multimodal sentiment analysis through reading comprehension},
  author = {Zhang, Hua and Yan, Yongjian and Cai, Zijing and Zhan, Peiqian and Chen, Bi and Jiang, Bo and Xie, Bo},
  journal = {Applied Soft Computing},
  volume = {167},
  pages = {112346},
  year = {2024},
  publisher = {Elsevier},
  abstract = {The primary challenge in multimodal sentiment analysis (MSA), which utilizes textual, audio, and visual information to analyze speakers' emotions, lies in constructing representation vectors that incorporate both unimodal semantic and multimodal interaction information. While existing research has extensively focused on multimodal fusion strategies, there remains insufficient exploration of the intrinsic potential within concurrently enhancing unimodal and multimodal representations. To address this gap, we introduce two additional steps to traditional three-step MSA: text modality enhancement through the machine reading comprehension (MRC) framework and multimodal representation reconstruction via proposing the diverse diffusion denoising autoencoder (D3AE) module. The MRC queries are integrated to locate sentiment-related prior knowledge, thereby deepening textual semantic understanding from a pretrained language model. Meanwhile, D3AE employs a single-step denoising strategy along with diffusion models across multiple time intervals, enabling efficient reconstruction and enhancement of multimodal representations. Extensive experiments conducted on two benchmark datasets, CMU-MOSI and CMU-MOSEI, validate that our model, MRC-D3AE, achieves state-of-the-art performance. The superiority of our model over existing baselines is primarily attributed to integrating MRC for enhancing text modality and D3AE for reconstructing multimodal representations.},
  doi = {10.1016/j.asoc.2024.112346}
}7:["$","div",null,{"className":"max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 py-12","children":[["$","$L15",null,{"config":{"type":"publication","title":"Publications","description":"A collection of my research work.","source":"publications.bib"},"publications":[{"id":"zhang2025reading","title":"Reading comprehension powered semantic fusion network for identification of N-ary drug combinations","authors":[{"name":"Hua Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Peiqian Zhan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Cheng Yang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yongjian Yan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zijing Cai","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Guogen Shan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Bo Jiang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Bi Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Qing Gu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Qingqing Zhou","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"journal","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:0:tags","researchArea":"machine-learning","journal":"Engineering Applications of Artificial Intelligence","conference":"","volume":"144","pages":"110096","doi":"https://doi.org/10.1016/j.engappai.2025.110096","code":"https://github.com/HelloNicoo/RCFIND","abstract":"$16","description":"","selected":false,"preview":"MRC-D3AE.png","bibtex":"$17"},{"id":"cai2024multi","title":"Multi-schema prompting powered token-feature woven attention network for short text classification","authors":[{"name":"Zijing Cai","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Hua Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Peiqian Zhan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xiaohui Jia","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yongjian Yan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xiawen Song","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Bo Xie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2024,"type":"journal","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:1:tags","researchArea":"transformer-architectures","journal":"Pattern Recognition","conference":"","volume":"156","pages":"110782","doi":"10.1016/j.patcog.2024.110782","code":"https://github.com/Aaronzijingcai/MP-TFWA","abstract":"$18","description":"","selected":true,"preview":"TFWAF.png","bibtex":"$19"},{"id":"zhang2024reconstructing","title":"Reconstructing representations using diffusion models for multimodal sentiment analysis through reading comprehension","authors":[{"name":"Hua Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yongjian Yan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zijing Cai","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Peiqian Zhan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Bi Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Bo Jiang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Bo Xie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2024,"type":"journal","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:2:tags","researchArea":"machine-learning","journal":"Applied Soft Computing","conference":"","volume":"167","pages":"112346","doi":"10.1016/j.asoc.2024.112346","code":"https://github.com/YYJ324/MRC-D3AE","abstract":"$1a","description":"","selected":false,"preview":"RCFIND.png","bibtex":"$1b"}]}],false,false]}]
c:null
10:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
b:null
14:{"metadata":[["$","title","0",{"children":"Publications | Zijing Cai"}],["$","meta","1",{"name":"description","content":"A collection of my research work."}],["$","meta","2",{"name":"author","content":"Zijing Cai"}],["$","meta","3",{"name":"keywords","content":"Zijing Cai,PhD,Research,USTC"}],["$","meta","4",{"name":"creator","content":"Zijing Cai"}],["$","meta","5",{"name":"publisher","content":"Zijing Cai"}],["$","meta","6",{"property":"og:title","content":"Zijing Cai"}],["$","meta","7",{"property":"og:description","content":"M.Eng. student at the USTC."}],["$","meta","8",{"property":"og:site_name","content":"Zijing Cai's Academic Website"}],["$","meta","9",{"property":"og:locale","content":"en_US"}],["$","meta","10",{"property":"og:type","content":"website"}],["$","meta","11",{"name":"twitter:card","content":"summary"}],["$","meta","12",{"name":"twitter:title","content":"Zijing Cai"}],["$","meta","13",{"name":"twitter:description","content":"M.Eng. student at the USTC."}],["$","link","14",{"rel":"icon","href":"/favicon.svg"}]],"error":null,"digest":"$undefined"}
e:{"metadata":"$14:metadata","error":null,"digest":"$undefined"}
