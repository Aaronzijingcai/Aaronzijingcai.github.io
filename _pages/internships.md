---
layout: archive
title: "Internships"
permalink: /internships/
author_profile: true
---

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:0;margin-left:0;font-size:1.0em;">
    <tbody>
        <tr>
            <td style="padding:8px;width:30%;vertical-align:middle;border:none;"> 
                <img src="https://github.com/Aaronzijingcai/Aaronzijingcai.github.io/tree/master/images" width="200">
            </td>


            <td style="padding:20px;width:70%;vertical-align:middle;border-right:none;border:none;">
                <b><a href="http://dev3.noahlab.com.hk/index.html">Huawei Noah's Ark Lab, Speech and Semantic Group</a></b>
                <br>
                <b>Research Intern</b>
                <br>
                <b>Sep. 2023 -- Feb. 2024, Hong Kong</b>
                <br>
                <b>Mentored by <a href="https://scholar.google.com/citations?user=gFoSqqkAAAAJ">Dr. Yufei Wang</a></b>
            </td>            
        </tr>
    </tbody>

</table>

<!--

## **Research Intern**

_**([Huawei Noah's Ark Lab, Speech and Semantic Group](http://dev3.noahlab.com.hk/index.html), Hong Kong, Sep. 2023 - Feb. 2024)**_
_**, mentored by [Dr. Yufei Wang](https://scholar.google.com/citations?user=gFoSqqkAAAAJ)**_
-->

* Instruction-Following Evaluation of LLMs: Constructed FollowBench, a multi-level fine-grained constraints following benchmark for systemically and precisely evaluating the instruction-following capability of LLMs.

* Multi-Turn Evaluation of LLMs: Introduced MT-Eval, a comprehensive benchmark designed to evaluate multi-turn conversational abilities of LLMs.

* Long-Context Evaluation of LLMs: Proposed M4LE, a Multi-ability, Multi-range, Multi-task, Multi-domain long-context evaluation benchmark, covering a wide range of tasks and domains across five context length buckets up to 12k.

* Knowledge Editing of LLMs: Proposed a Learning to Edit (LTE) framework for effective and efficient knowledge editing of LLMs.

